from preproccessing.HackatonUtils import *
import numpy as np
from preproccessing.utils import *

HEADER = "HEADER    IMMUNE SYSTEM - NANOBODY                           \nTITLE     COMPUTATIONAL MODELING     \nREMARK 777 MODEL GENERATED BY NANONET \n"
ATOM_LINE = "ATOM{}{}  CA  {} H{}{}{}{:.3f}{}{:.3f}{}{:.3f}  1.00 0.00           C\n"
END_LINE = "END"
CLUSTER_PREFIX = ">Cluster "


def matrix_to_pdb(pdb_file, seq, coord_matrix):
    """
    Translates a matrix of Ca x,y,z coordinates to PDB format
    :param pdb_file: pdb file to write to
    :param seq: Nb sequence
    :param coord_matrix: NanoNet output
    :return: None
    """
    pdb_file.write(HEADER)
    i = 1
    for aa in range(len(seq)):
        if seq[aa] != "-":
            first_space = (7 - len(str(i))) * " "
            second_space = (4 - len(str(i))) * " "
            third_space = (12 - len("{:.3f}".format(coord_matrix[aa][0]))) * " "
            forth_space = (8 - len("{:.3f}".format(coord_matrix[aa][1]))) * " "
            fifth_space = (8 - len("{:.3f}".format(coord_matrix[aa][2]))) * " "
            if seq[aa] == "X":
                three = "UNK"
            else:
                three = Polypeptide.one_to_three(seq[aa])
            pdb_file.write(
                ATOM_LINE.format(first_space, i, three, second_space, i, third_space, coord_matrix[aa][0], forth_space,
                                 coord_matrix[aa][1], fifth_space, coord_matrix[aa][2]))
            i += 1
    pdb_file.write(END_LINE)


def createRandomPDBs(path_to_matrix, num):
    """
    Receives a path to coordinates matrix of 100,000 different nanobodies and create 1,000 random PDBs
    """

    indexes = list(np.random.choice(np.arange(1, 100001), 1000, replace=False))

    sequences = []
    seq = open("../data_files/sequences.txt", 'r').read().split('\n')[(num - 1) * 100000: num * 100000]

    for index in indexes:
        sequences.append(seq[index - 1])

    #   load matrix with aa coordinates
    mat = np.load(path_to_matrix)

    file_with_real_indexes = open('indexes.txt', 'a')

    #    create ca pdb file
    for j in range(len(indexes)):
        ca_file_name = "pdbs/" + str((num - 1) * 1000 + j + 1) + ".pdb"
        file_with_real_indexes.write(str((num - 1) * 100000 + indexes[j]) + "\n")
        with open(ca_file_name, "w") as ca_file:
            matrix_to_pdb(ca_file, sequences[j], mat[indexes[j] - 1])
            print(mat[indexes[j] - 1])
        if (j == 0):
            print(sequences[j])
            print(indexes[j])


def parse_DCHit(path_to_clstrs, dict_path):
    """
    parse the CD-Hit clusters and create pdbs of one of 2 options (need to uncomment the option):
    1) 1000 ramdomm PDB's from the biggest cluster
    or
    2) 5 random PDB's from one of each of the 200 biggest clusters

    :param path_to_clstrs: path to the output file of DCHit
    :param dict_path: path to a dictionary that translate nanobody name <-> nanobody index
    :return: None
    """
    name = replace_names_cluster_files(path_to_clstrs, dict_path)
    res = open(path_to_clstrs + name, 'r').read().split('\n')
    res = res[:len(res) - 1]
    cluster_num = -1
    clusters = {}
    for i in range(len(res)):
        if res[i].find(CLUSTER_PREFIX) >= 0:
            cluster_num = int(res[i][len(CLUSTER_PREFIX):])
        else:
            if (cluster_num in clusters):
                clusters[cluster_num].append(int(res[i]))
            else:
                clusters[cluster_num] = [int(res[i])]

    # sorting the clusters by size
    max_cluster = 0
    max_size = 0
    num_of_clusters = 0
    small_clusters = 0
    for key in clusters:
        num_of_clusters += 1
        if (len(clusters[key]) > max_size):
            max_cluster = key
            max_size = len(clusters[key])
        if(len(clusters[key]) < 3):
            small_clusters +=1

    clusters_by_size = sorted(clusters, key=lambda k: len(clusters[k]), reverse=True)
    print("num of clusters: " + str(num_of_clusters))
    print("num of small clusters: " + str(small_clusters))

    for i in range(10):
        print(len(clusters[clusters_by_size[i]]))

    # OPTION 1:
    #   5 random pdbs from 200 biggest clusters
    #
    #
    f = open("pdbs/pdbs_5_from_200.txt", 'a')
    for i in range(200):
        indexes = np.random.choice(np.arange(len(clusters[clusters_by_size[i]])), 5, replace= False)
        global_indexes = np.array(np.array(clusters[clusters_by_size[i]])[indexes])
        creaPDBs_by_index(global_indexes, "pdbs/pdbs_5_from_200/")
        f.write(str(global_indexes) + '\n')
    f.close()

    # OPTION 2:
    #   1000 random pdbs from biggest cluster
    #
    #
    indexes = list(np.random.choice(np.arange(0,len(clusters[max_cluster])), 1000, replace = False))
    creaPDBs_by_index(np.array(clusters[max_cluster])[indexes], "pdbs/pdbs_1000_biggest/")


def creaPDBs_by_index(indexes, path):
    """
    :param indexes: indexes of nanobody's we ant to create thier PDBs
    :param path: path to the directory we want the PDB's to be saved in
    :return: None
    """
    sequences = open("sequences.txt", 'r').read().split('\n')

    for i in range(1, 10):
        mat = np.load("pdb_results/" + str(i) + "_100000.npy", 'r')
        filtered_ind = indexes[indexes < i * 100000]
        filtered_ind = filtered_ind[filtered_ind > (i - 1) * 100000]
        for ind in filtered_ind:
            ca_file_name = path + str(ind) + ".pdb"
            with open(ca_file_name, "w") as ca_file:
                matrix_to_pdb(ca_file, sequences[ind - 1], mat[(ind - 1) % 100000])


if __name__ == '__main__':
    """
    Main function for parsing CD-Hit and creating PDB files in some m
    """
    #   parse CDHits results - 2 options:
    #   1. create 1000 random pdbs from biggest cluster
    #   2. create 5 random pdbs from each of the biggest 200 clusters
    PATH_TO_CD_HIT_OUTPUT = ""
    PATH_TO_DICT = ""
    parse_DCHit(PATH_TO_CD_HIT_OUTPUT, PATH_TO_DICT)
