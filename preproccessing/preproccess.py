from preproccessing.HackatonUtils import *
import argparse
import os
import numpy as np
from preproccessing.utils import *

HEADER = "HEADER    IMMUNE SYSTEM - NANOBODY                           \nTITLE     COMPUTATIONAL MODELING     \nREMARK 777 MODEL GENERATED BY NANONET \n"
ATOM_LINE = "ATOM{}{}  CA  {} H{}{}{}{:.3f}{}{:.3f}{}{:.3f}  1.00 0.00           C\n"
END_LINE = "END"
CLUSTER_PREFIX = ">Cluster "

def matrix_to_pdb(pdb_file, seq, coord_matrix):
    """
    translates a matrix of Ca x,y,z coordinates to PDB format
    :param pdb_file: pdb file to write to
    :param seq: Nb sequence
    :param coord_matrix: NanoNet output
    :return: None
    """
    pdb_file.write(HEADER)
    i = 1
    for aa in range(len(seq)):
        if seq[aa] != "-":
            first_space = (7 - len(str(i))) * " "
            second_space = (4 - len(str(i))) * " "
            third_space = (12 - len("{:.3f}".format(coord_matrix[aa][0]))) * " "
            forth_space = (8 - len("{:.3f}".format(coord_matrix[aa][1]))) * " "
            fifth_space = (8 - len("{:.3f}".format(coord_matrix[aa][2]))) * " "
            if seq[aa] == "X":
                three = "UNK"
            else:
                three = Polypeptide.one_to_three(seq[aa])
            pdb_file.write(ATOM_LINE.format(first_space, i, three, second_space, i, third_space, coord_matrix[aa][0],forth_space, coord_matrix[aa][1],fifth_space, coord_matrix[aa][2]))
            i += 1
    pdb_file.write(END_LINE)


# def createRandomPDBs(path_to_matrix, num):
#     """
#     recieves a path to coordinates matrix of 100,000 different nanobodies and create 10,000 random PDBs
#     """
#
#     # TODO: 1. get 100,000 random pdbs and save their indexes
#     #      2. think of which clusters to save (remove small clusters)
#
#     indexes = list(np.random.choice(np.arange(1,100001), 10000, replace=False))
#
#     sequences = []
#     seq = open("sequences.txt", 'r').read().split('\n')[(num-1) * 100000: num * 100000]
#
#     for index in indexes:
#         sequences.append(seq[index - 1])
#
#
#     print(len(sequences))
#
#     # load matrix with aa coordinates
#     mat = np.load(path_to_matrix)
#
#     # create ca pdb file
#     for j in range(len(indexes)):
#         ca_file_name = "pdbss/" + str((num-1)*100000 + indexes[j]) + ".pdb"
#         with open(ca_file_name, "w") as ca_file:
#             matrix_to_pdb(ca_file, sequences[j], mat[indexes[j] - 1])
#         if(j == 0):
#             print(sequences[j])
#             print(indexes[j])

def createRandomPDBs(path_to_matrix, num):
    """
    recieves a path to coordinates matrix of 100,000 different nanobodies and create 1,000 random PDBs
    """

    # TODO: 1. get 100,000 random pdbs and save their indexes
    #      2. think of which clusters to save (remove small clusters)

    indexes = list(np.random.choice(np.arange(1,100001), 1000, replace=False))

    sequences = []
    seq = open("../data_files/sequences.txt", 'r').read().split('\n')[(num - 1) * 100000: num * 100000]

    for index in indexes:
        sequences.append(seq[index - 1])

    #   load matrix with aa coordinates
    mat = np.load(path_to_matrix)

    file_with_real_indexes = open('indexes.txt', 'a')

    #    create ca pdb file
    for j in range(len(indexes)):
        ca_file_name = "pdbs/" + str((num-1)*1000 + j + 1) + ".pdb"
        file_with_real_indexes.write(str((num-1)*100000 + indexes[j]) + "\n")
        with open(ca_file_name, "w") as ca_file:
            matrix_to_pdb(ca_file, sequences[j], mat[indexes[j] - 1])
            print(mat[indexes[j] - 1])
        if(j == 0):
            print(sequences[j])
            print(indexes[j])



def parse_DCHit(path_to_clstrs):
    """
    recieves a path to the output file of DCHit
    """

    name = replace_names_cluster_files(path_to_clstrs , "/cs/usr/fridalon/PycharmProjects/3dbio_hackathon/data_files/name_num_dict")
    res = open(path_to_clstrs + name, 'r').read().split('\n')
    res = res[:len(res) - 1]
    cluster_num = -1
    clusters = {}
    for i in range(len(res)):
        if res[i].find(CLUSTER_PREFIX) >= 0:
            cluster_num = int(res[i][len(CLUSTER_PREFIX):])
        else:
            if(cluster_num in clusters):
                clusters[cluster_num].append(int(res[i]))
            else:
                clusters[cluster_num] = [int(res[i])]

    max_size = 0
    max_cluster = 0
    for key in clusters:
        if(len(clusters[key]) > max_size):
            max_cluster = key
            max_size = len(clusters[key])

    indexes = list(np.random.choice(np.arange(0,len(clusters[max_cluster])), 1000, replace = False))
    return np.array(clusters[max_cluster])[indexes]


def createPDBs():
    for num in range(1,10):
        createRandomPDBs("/cs/usr/fridalon/Downloads/" + str(num) + "_100000.npy", num)

# def creaPDBs_by_index(indexes):
#
#     sequences = open("../data_files/sequences.txt", 'r').read().split('\n')
#
#     for j in range(len(indexes)):
#         ca_file_name = "pdbs/1000_from_biggest/" + str(indexes[j]) + ".pdb"
#         with open(ca_file_name, "w") as ca_file:
#             matrix_to_pdb(ca_file, sequences[indexes[j]], mat[indexes[j] - 1])
#             print(mat[indexes[j] - 1])
#         if (j == 0):
#             print(sequences[j])
#             print(indexes[j])


if __name__ == '__main__':
    """
    receives path to a Nb fasta file and a path to a trained neural network and creates a pdb file (Ca only) according to
    the network prediction. the output file name is: "<fasta file name>_nanonet_ca.pdb"
    """

    indexes = parse_DCHit("CDHits_results/")
    print(indexes)





